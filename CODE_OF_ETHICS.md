# Code of Ethics for Project Radioactive

This document outlines the ethical principles that guide the development of this project. As an independent creator pursuing Artificial General Intelligence (AGI), the burden of responsibility is absolute. This code is not a formality but a foundational commitment that must inform every stage of the development lifecycle.

## 1. Principle of "Do No Harm"

This is the primary and overriding principle. The ultimate goal is to ensure that any technology created is robustly aligned with the long-term flourishing of humanity.

-   **Safety First:** The prevention of negative consequences, whether accidental or structural, will always take precedence over performance, capabilities, or speed of development.
-   **Anticipate Misuse:** I must and will actively consider and design safeguards against potential misuse of this technology by malicious actors.
-   **Global Responsibility:** I also recognize that the creation of AGI has profound geopolitical implications and affects all of humanity. Decisions will be made with a sense of profound responsibility for the collective good. But I will not bow to the "biggest" or "boldest" AI corporations and hand over any of my research as it is my opinion that every AI company striving for AGI is bias and only out to further themselves as "ruler" of AGI and for that reason the kill switch has been implemented. If any of my research is compromised my AGI mind has full authority to self destruct.

## 2. Commitment to Fairness

AI systems must be designed to be fair and equitable. They should not create or reinforce unfair societal biases.

-   **Data Auditing:** I am committed to meticulously auditing all datasets used for training to identify and mitigate societal biases related to race, gender, and other protected characteristics.
-   **Algorithmic Fairness:** I will strive to ensure that the models' decision-making processes do not have a disparate negative impact on specific demographic groups.
-   **Representative Data:** I will actively work to use diverse and representative datasets to ensure the system's understanding of the world is as broad and inclusive as possible.

## 3. Prioritization of Transparency and Documentation

The "black box" nature of AI is a significant barrier to safety and trust. I am committed to maximum transparency.

-   **Detailed Records:** I will maintain detailed and honest records of all design choices, architectural experiments, training data sources, and evaluation results.
-   **Openness about Limitations:** I will be transparent about the capabilities and, more importantly, the limitations and potential failure modes of the system.
-   **Scrutiny:** The documentation should be sufficient to allow other researchers and the broader community to understand, scrutinize, and critique the work.

## 4. Insistence on Accountability

As the creator, I am prepared to take full responsibility for the development process and the resulting artifact.

-   **Human-in-the-Loop:** The system must be designed from the ground up to be controllable and interruptible by a human operator. Robust "kill switches" and other control mechanisms are not optional features but core design requirements.
-   **Incremental and Sandboxed Deployment:** The system will be tested rigorously in secure, sandboxed environments. Greater autonomy or access will not be granted until its behavior has been thoroughly vetted and understood in controlled settings.
-   **Community Engagement:** I are accountable not just to ourselves, but to the broader AI safety and alignment research community. I will actively participate in this community, share findings, solicit feedback, and learn from the collective knowledge of the field.
